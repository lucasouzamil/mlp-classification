{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Classifica\u00e7\u00e3o com MLP","text":""},{"location":"#classificacao-com-mlp","title":"Classifica\u00e7\u00e3o com MLP","text":""},{"location":"#grupo","title":"Grupo","text":"<ol> <li>Eduardo Selber Castanho</li> <li>Henrique Fazzio Badin</li> <li>Lucas Fernando de Souza Lima</li> </ol> <p>Arquivo Jupyter implementando o MLP .</p>"},{"location":"#1-dataset-overview","title":"1. Dataset Overview","text":"<p>Dataset: Predict Students' Dropout and Academic Success Fonte: UCI Machine Learning Repository Dom\u00ednio: Educa\u00e7\u00e3o / Ci\u00eancias Sociais Tipo de dados: Tabular (num\u00e9ricos, categ\u00f3ricos e inteiros) Tarefa: Classifica\u00e7\u00e3o multiclasse (3 classes: dropout, enrolled, graduate) Amostras: 4.424 Features: 36 Descri\u00e7\u00e3o: Dados de estudantes de gradua\u00e7\u00e3o em diversas \u00e1reas, contendo informa\u00e7\u00f5es demogr\u00e1ficas, socioecon\u00f4micas e de desempenho acad\u00eamico nos dois primeiros semestres. O objetivo \u00e9 prever a situa\u00e7\u00e3o final do aluno \u2014 evas\u00e3o, matr\u00edcula ativa ou gradua\u00e7\u00e3o.  </p> <p>Motiva\u00e7\u00e3o: O Predict Students' Dropout and Academic Success foi escolhido por ser tamb\u00e9m utilizado em uma competi\u00e7\u00e3o no Kaggle, o que refor\u00e7a sua relev\u00e2ncia e permite compara\u00e7\u00f5es de desempenho entre diferentes abordagens de classifica\u00e7\u00e3o.  </p> <p>Al\u00e9m disso, trata-se de um tema socialmente importante, pois os resultados obtidos podem ser aplicados em escolas, universidades e institui\u00e7\u00f5es educacionais para identificar precocemente alunos com risco de evas\u00e3o e orientar estrat\u00e9gias de apoio acad\u00eamico.  </p> <p>O dataset \u00e9 robusto (mais de 4.000 inst\u00e2ncias e 36 vari\u00e1veis) e, ao mesmo tempo, bem estruturado, com dados limpos e sem valores ausentes ou duplicados, al\u00e9m de descri\u00e7\u00f5es claras das features, o que facilita o desenvolvimento e an\u00e1lise de modelos de aprendizado de m\u00e1quina.  </p>"},{"location":"#2-dataset-explanation","title":"2. Dataset Explanation","text":"<p>O conjunto de dados Predict Students\u2019 Dropout and Academic Success cont\u00e9m informa\u00e7\u00f5es acad\u00eamicas, demogr\u00e1ficas e socioecon\u00f4micas de estudantes de cursos de gradua\u00e7\u00e3o, coletadas no momento da matr\u00edcula e ao final dos dois primeiros semestres. O objetivo \u00e9 prever o status final do aluno \u2014 dropout, enrolled ou graduate \u2014 configurando um problema de classifica\u00e7\u00e3o multiclasse.  </p> <p>O dataset possui 36 vari\u00e1veis e 4.424 inst\u00e2ncias, todas sem valores nulos ou duplicados. As features incluem dados pessoais, hist\u00f3rico acad\u00eamico, desempenho em disciplinas e indicadores macroecon\u00f4micos.  </p>"},{"location":"#21-feature-description","title":"2.1 Feature Description","text":"Feature Tipo Descri\u00e7\u00e3o Valores / Intervalos Marital Status Integer Estado civil 1\u2013single, 2\u2013married, 3\u2013widower, 4\u2013divorced, 5\u2013facto union, 6\u2013legally separated Application mode Integer Tipo de candidatura ao curso 1\u2013general, 5\u2013special contingent, 17\u20132nd phase, etc. Application order Integer Ordem de prefer\u00eancia da candidatura 0\u20139 Course Integer Curso matriculado V\u00e1rios c\u00f3digos (ex.: 9147\u2013Management, 9500\u2013Nursing) Daytime/evening attendance Integer Turno 1\u2013daytime, 0\u2013evening Previous qualification Integer N\u00edvel de escolaridade anterior 1\u2013secondary, 2\u2013higher ed., etc. Previous qualification (grade) Continuous Nota da qualifica\u00e7\u00e3o anterior 0\u2013200 Nationality Integer Nacionalidade Ex.: 1\u2013Portuguese, 41\u2013Brazilian, 21\u2013Angolan Mother\u2019s qualification Integer Escolaridade da m\u00e3e 1\u2013secondary, 5\u2013doctorate, etc. Father\u2019s qualification Integer Escolaridade do pai 1\u2013secondary, 5\u2013doctorate, etc. Mother\u2019s occupation Integer Ocupa\u00e7\u00e3o da m\u00e3e 0\u2013student, 1\u2013manager, 5\u2013service worker, etc. Father\u2019s occupation Integer Ocupa\u00e7\u00e3o do pai 0\u2013student, 1\u2013manager, 8\u2013technician, etc. Admission grade Continuous Nota de admiss\u00e3o no curso 0\u2013200 Displaced Integer Mora fora da cidade de origem 1\u2013yes, 0\u2013no Educational special needs Integer Necessidades educacionais especiais 1\u2013yes, 0\u2013no Debtor Integer Est\u00e1 em d\u00e9bito financeiro 1\u2013yes, 0\u2013no Tuition fees up to date Integer Mensalidades em dia 1\u2013yes, 0\u2013no Gender Integer Sexo do estudante 1\u2013male, 0\u2013female Scholarship holder Integer Possui bolsa de estudos 1\u2013yes, 0\u2013no Age at enrollment Integer Idade no momento da matr\u00edcula anos International Integer Estudante internacional 1\u2013yes, 0\u2013no Curricular units 1st sem (credited) Integer Disciplinas creditadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (enrolled) Integer Disciplinas matriculadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (evaluations) Integer Avalia\u00e7\u00f5es realizadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (approved) Integer Disciplinas aprovadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (grade) Integer M\u00e9dia de notas (1\u00ba semestre) 0\u201320 Curricular units 1st sem (without evaluations) Integer Disciplinas sem avalia\u00e7\u00e3o (1\u00ba semestre) 0\u2013n Curricular units 2nd sem (credited) Integer Disciplinas creditadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (enrolled) Integer Disciplinas matriculadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (evaluations) Integer Avalia\u00e7\u00f5es realizadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (approved) Integer Disciplinas aprovadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (grade) Integer M\u00e9dia de notas (2\u00ba semestre) 0\u201320 Curricular units 2nd sem (without evaluations) Integer Disciplinas sem avalia\u00e7\u00e3o (2\u00ba semestre) 0\u2013n Unemployment rate Continuous Taxa de desemprego (%) valor cont\u00ednuo Inflation rate Continuous Taxa de infla\u00e7\u00e3o (%) valor cont\u00ednuo GDP Continuous Produto Interno Bruto valor cont\u00ednuo Target Categorical Situa\u00e7\u00e3o final do aluno Dropout, Enrolled, Graduate"},{"location":"#22-target-variable","title":"2.2 Target Variable","text":"<p>A vari\u00e1vel Target representa a situa\u00e7\u00e3o final do estudante ao t\u00e9rmino do curso:</p> <ul> <li>Dropout (0): o aluno abandonou o curso  </li> <li>Enrolled (1): o aluno ainda est\u00e1 matriculado  </li> <li>Graduate (2): o aluno concluiu o curso com sucesso</li> </ul>"},{"location":"#23-data-issues","title":"2.3 Data Issues","text":"<p>O dataset j\u00e1 foi pr\u00e9-processado e limpo pelos autores originais, n\u00e3o apresentando:</p> <ul> <li>Valores ausentes (<code>NaN</code>);</li> <li>Duplicatas;</li> <li>Outliers sem explica\u00e7\u00e3o evidente.  </li> </ul>"},{"location":"#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization","text":"<p>A etapa de limpeza e normaliza\u00e7\u00e3o foi realizada para garantir a qualidade dos dados e preparar o conjunto para o treinamento da rede MLP. O processo incluiu inspe\u00e7\u00e3o inicial, remo\u00e7\u00e3o de outliers, codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas, normaliza\u00e7\u00e3o num\u00e9rica e an\u00e1lise explorat\u00f3ria com PCA (Principal Component Analysis).</p>"},{"location":"#31-initial-inspection","title":"3.1 Initial Inspection","text":"<p>Ap\u00f3s o carregamento do dataset (<code>train.csv</code>), foi feita uma an\u00e1lise explorat\u00f3ria inicial com <code>pandas</code> e <code>matplotlib</code> para verificar estrutura, tipos de dados e poss\u00edveis problemas de consist\u00eancia.</p> <pre><code>df.info()\ndf.isnull().sum()\ndf.describe().transpose()\ndf.hist(bins=30, figsize=(20, 15))\n</code></pre> <ul> <li>Nenhum valor nulo foi encontrado.</li> <li>N\u00e3o h\u00e1 colunas duplicadas.</li> <li>As distribui\u00e7\u00f5es mostram amplitudes diferentes entre vari\u00e1veis, o que justifica a posterior normaliza\u00e7\u00e3o.</li> </ul> <p>Figura 1 \u2014 Distribui\u00e7\u00e3o inicial das features</p> <p></p>"},{"location":"#32-outlier-detection-and-removal","title":"3.2 Outlier Detection and Removal","text":"<p>Foram definidos limites manuais de plausibilidade (bounds) para cada vari\u00e1vel quantitativa, baseados em conhecimento de dom\u00ednio e na distribui\u00e7\u00e3o dos dados. A fun\u00e7\u00e3o <code>remove_outliers_by_bounds()</code> filtrou linhas fora desses intervalos.</p> <pre><code>df_sem_outliers = remove_outliers_by_bounds(df)\n</code></pre>"},{"location":"#principais-resultados","title":"Principais resultados:","text":"Vari\u00e1vel Intervalo Mantido Linhas Removidas Previous qualification (grade) [80, 180] 13 Admission grade [90, 180] 16 Age at enrollment [16, 60] 33 Curricular units 1st sem (approved) [0, 15] 108 Curricular units 1st sem (without evaluations) [0, 4] 127 Curricular units 2nd sem (without evaluations) [0, 1] 1.011 Inflation rate [-1.5, 3.5] 7.327 GDP [-4.5, 3.5] 5.082 <p>Resumo final: 76.518 \u2192 62.502 linhas ap\u00f3s limpeza.</p> <p>Figura 2 \u2014 Distribui\u00e7\u00e3o ap\u00f3s remo\u00e7\u00e3o de outliers</p> <p></p> <p>A filtragem removeu registros com valores extremos (principalmente em indicadores macroecon\u00f4micos e notas m\u00e9dias), reduzindo ru\u00eddos sem comprometer o volume de dados.</p>"},{"location":"#33-feature-encoding","title":"3.3 Feature Encoding","text":"<p>Para transformar vari\u00e1veis categ\u00f3ricas em num\u00e9ricas, foi aplicada codifica\u00e7\u00e3o one-hot com <code>pandas.get_dummies()</code>, preservando todas as categorias:</p> <pre><code>df_encoded = pd.get_dummies(df, columns=[\n    \"Nacionality\", \"Marital status\", \"Application mode\", \"Course\",\n    \"Previous qualification\", \"Mother's qualification\", \"Father's qualification\",\n    \"Mother's occupation\", \"Father's occupation\"\n])\n</code></pre> <ul> <li>As novas colunas (<code>uint8</code>) representaram corretamente cada categoria.</li> <li> <p>O <code>Target</code> foi mapeado para valores num\u00e9ricos:</p> </li> <li> <p><code>Graduate = 1.0</code></p> </li> <li><code>Dropout = 0.5</code></li> <li><code>Enrolled = 0.0</code></li> </ul>"},{"location":"#34-normalization","title":"3.4 Normalization","text":"<p>Como as features possuem escalas diferentes (por exemplo, notas entre 0\u2013200 e idades entre 16\u201360), aplicou-se a normaliza\u00e7\u00e3o Min\u2013Max, escalando todos os valores para o intervalo [0, 1]:</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_scaled[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n</code></pre> <p>Essa transforma\u00e7\u00e3o assegura que todas as vari\u00e1veis tenham o mesmo peso relativo no c\u00e1lculo dos gradientes durante o treinamento do MLP.</p>"},{"location":"#35-dimensionality-reduction-pca","title":"3.5 Dimensionality Reduction (PCA)","text":"<p>Devido ao grande n\u00famero de vari\u00e1veis presentes no dataset \u2014 ap\u00f3s o one-hot encoding havia centenas de features num\u00e9ricas \u2014 aplicamos uma An\u00e1lise de Componentes Principais (PCA) com o objetivo de reduzir a dimensionalidade dos dados e facilitar a visualiza\u00e7\u00e3o da separabilidade entre as classes.</p> <p>A redu\u00e7\u00e3o de dimensionalidade \u00e9 \u00fatil neste caso porque: - Diminui a complexidade computacional para an\u00e1lises explorat\u00f3rias. - Permite observar padr\u00f5es e agrupamentos de classes em um espa\u00e7o bidimensional. - Ajuda a verificar se h\u00e1 algum grau de separa\u00e7\u00e3o natural entre as classes antes do treinamento do MLP.</p> <p>O PCA foi aplicado sobre as colunas num\u00e9ricas normalizadas, extraindo os dois primeiros componentes principais:</p> <pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_result = pca.fit_transform(df_scaled[numeric_cols])\n</code></pre> <p>Os resultados mostraram que:</p> <ul> <li>PC1: explica aproximadamente 34% da vari\u00e2ncia total</li> <li>PC2: explica aproximadamente 21% da vari\u00e2ncia</li> <li>Total: cerca de 55% da vari\u00e2ncia foi capturada pelos dois primeiros componentes</li> </ul> <p>Isso indica que, embora a maior parte da informa\u00e7\u00e3o ainda esteja distribu\u00edda entre diversas dimens\u00f5es, \u00e9 poss\u00edvel visualizar parte da estrutura dos dados em duas dimens\u00f5es.</p> <p>A figura abaixo mostra a proje\u00e7\u00e3o dos dados no plano formado pelos dois primeiros componentes principais, com as classes representadas por cores diferentes.</p> <p>Figura 3 \u2014 PCA (PC1 vs PC2) colorido por classe</p> <p></p>"},{"location":"#36-summary","title":"3.6 Summary","text":"<ul> <li>Nenhum dado ausente ou duplicado encontrado.</li> <li>Outliers removidos com base em limites definidos manualmente.</li> <li>Vari\u00e1veis categ\u00f3ricas convertidas por one-hot encoding.</li> <li>Normaliza\u00e7\u00e3o Min\u2013Max aplicada a todas as vari\u00e1veis num\u00e9ricas.</li> </ul>"},{"location":"#4-mlp-implementation","title":"4. MLP Implementation","text":"<p>Para a implementa\u00e7\u00e3o do MLP consideramos decis\u00f5es de design sobre arquitetura, fun\u00e7\u00f5es de ativa\u00e7\u00e3o, fun\u00e7\u00e3o de perda, inicializa\u00e7\u00e3o de pesos e hiperpar\u00e2metros. A implementa\u00e7\u00e3o foi feita em NumPy para refor\u00e7ar a compreens\u00e3o das opera\u00e7\u00f5es fundamentais (forward, loss, backprop e atualiza\u00e7\u00e3o de pesos). Em seguida detalhamos cada componente.</p>"},{"location":"#41-network-architecture","title":"4.1 Network Architecture","text":"<p>Optou-se por um MLP com duas camadas ocultas:</p> <ul> <li>Input: dimens\u00e3o <code>D</code> (n\u00famero de features ap\u00f3s one-hot + normaliza\u00e7\u00e3o).</li> <li>Hidden layer 1: <code>H1 = 64</code> neur\u00f4nios, ativa\u00e7\u00e3o <code>tanh</code>.</li> <li>Hidden layer 2: <code>H2 = 32</code> neur\u00f4nios, ativa\u00e7\u00e3o <code>tanh</code>.</li> <li>Output: <code>K</code> neur\u00f4nios (n\u00famero de classes), ativa\u00e7\u00e3o <code>softmax</code>.</li> </ul> <p>Justificativa curta:</p> <ul> <li>Duas camadas ocultas capturam n\u00e3o linearidades mais complexas que uma \u00fanica camada sem tornar a rede excessivamente profunda.</li> <li>H1/H2 escolhidos por experimenta\u00e7\u00e3o emp\u00edrica: aumentos significativos na largura n\u00e3o trouxeram ganhos relevantes, apenas custo computacional.</li> </ul>"},{"location":"#42-activation-functions","title":"4.2 Activation Functions","text":"<ul> <li> <p>Camadas ocultas: <code>tanh</code></p> </li> <li> <p>Vantagem: sa\u00edda centrada em zero, derivada simples (<code>1 - tanh^2</code>) e bom comportamento com inicializa\u00e7\u00f5es escalonadas.</p> </li> <li>Observa\u00e7\u00e3o: <code>ReLU</code> pode acelerar converg\u00eancia, mas neste trabalho mantivemos <code>tanh</code> por estabilidade e por corresponder ao material do curso.</li> <li> <p>Camada de sa\u00edda: <code>softmax</code></p> </li> <li> <p>Produz probabilidades normalizadas por amostra, adequada para cross-entropy multiclasses.</p> </li> </ul>"},{"location":"#43-loss-function","title":"4.3 Loss Function","text":"<ul> <li>Cross-entropy (categorical) combinada com regulariza\u00e7\u00e3o L2 aplicada a todos os pesos:</li> </ul> \\[ \\text{loss} = -\\frac{1}{B}\\sum_{i=1}^{B}\\sum_{k=1}^{K} y_{ik}\\log(\\hat{y}_{ik} + \\varepsilon) + \\lambda \\sum_{\\ell} \\|W^{(\\ell)}\\|^2 \\] <ul> <li><code>B</code> \u00e9 o tamanho do mini-batch.</li> <li> <p>A combina\u00e7\u00e3o cross-entropy + softmax simplifica a derivada do output para <code>(yhat - y_onehot)/B</code>, o que facilita o c\u00e1lculo de gradientes.</p> </li> <li> <p>Motiva\u00e7\u00e3o: cross-entropy penaliza previs\u00f5es confiantes e erradas de forma mais forte que MSE, sendo padr\u00e3o em classifica\u00e7\u00e3o probabil\u00edstica.</p> </li> </ul>"},{"location":"#44-hyperparameters","title":"4.4 Hyperparameters","text":"<p>Hiperpar\u00e2metros selecionados (valores usados nos experimentos):</p> <ul> <li> <p>Learning Rate (lr): <code>0.005</code></p> </li> <li> <p>Testes com 0.001, 0.005 e 0.01 indicaram que 0.005 entregou bom equil\u00edbrio entre velocidade e estabilidade com <code>tanh</code>.</p> </li> <li> <p>Epochs (m\u00e1x): <code>500</code></p> </li> <li> <p>Limite superior; o treinamento \u00e9 interrompido via early stopping quando a valida\u00e7\u00e3o n\u00e3o melhora.</p> </li> <li> <p>Batch size: <code>64</code></p> </li> <li> <p>Atualiza\u00e7\u00e3o mais frequente que <code>128</code>, com bom trade-off entre estabilidade do gradiente e varia\u00e7\u00e3o estoc\u00e1stica \u00fatil.</p> </li> <li> <p>Regularization (L2): <code>lambda_l2 = 1e-4</code></p> </li> <li> <p>Penaliza\u00e7\u00e3o leve que reduz overfitting sem prejudicar aprendizagem.</p> </li> <li>Patience (early stopping): <code>15</code> \u00e9pocas sem melhora em <code>val_loss</code>.</li> <li> <p>Inicializa\u00e7\u00e3o de pesos: normal padr\u00e3o com escala <code>1/sqrt(fan_in)</code>:</p> </li> <li> <p>ex.: <code>W = rng.normal(0,1,(out,in)) / sqrt(in)</code> \u2014 reduz satura\u00e7\u00e3o inicial e ajuda na estabilidade do treino.</p> </li> <li>Seed / reproducibility: <code>np.random.default_rng(42)</code> e <code>random_state=42</code> nos splits.</li> </ul> <p>Ajustes e recomenda\u00e7\u00f5es:</p> <ul> <li>Para acelerar converg\u00eancia pode-se testar <code>ReLU + Adam</code> (optimizers modernos) e <code>batch normalization</code>.</li> <li>Para lidar com desbalanceamento usar <code>class_weight</code> no loss ou t\u00e9cnicas de oversampling (SMOTE).</li> </ul>"},{"location":"#45-implementation-details","title":"4.5 Implementation Details","text":"<p>A implementa\u00e7\u00e3o do MLP foi feita inteiramente em NumPy, permitindo controle direto sobre cada etapa \u2014 forward pass, c\u00e1lculo da fun\u00e7\u00e3o de perda, retropropaga\u00e7\u00e3o e atualiza\u00e7\u00e3o dos par\u00e2metros. Essa abordagem possibilita compreender o fluxo interno de dados e gradientes, sem a abstra\u00e7\u00e3o de frameworks como PyTorch ou TensorFlow.</p> <p>Para refer\u00eancia, as etapas de treinamento seguiram o fluxo do material de Numerical Simulation do curso, dividido em:</p> <ol> <li>Forward Pass: os dados s\u00e3o propagados camada a camada;</li> <li>Loss Calculation: c\u00e1lculo da cross-entropy e penaliza\u00e7\u00e3o L2;</li> <li>Backward Pass: propaga\u00e7\u00e3o reversa dos gradientes para ajustar pesos e vieses.</li> </ol> <pre><code># Forward Pass (propaga\u00e7\u00e3o)\nz1 = xb @ W1.T + b1; h1 = tanh(z1)\nz2 = h1 @ W2.T + b2; h2 = tanh(z2)\nz3 = h2 @ W3.T + b3; yhat = softmax(z3)\n\n# C\u00e1lculo da Loss (cross-entropy + L2)\nloss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))\nl2_term = lambda_l2 * (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))\nloss = loss_ce + l2_term\n\n# Backpropagation\nd3 = (yhat - yb) / B\ngW3 = d3.T @ h2 + 2*lambda_l2*W3\ngb3 = d3.sum(axis=0)\n\ndh2 = d3 @ W3\ndz2 = dh2 * (1 - h2**2)\ngW2 = dz2.T @ h1 + 2*lambda_l2*W2\ngb2 = dz2.sum(axis=0)\n\ndh1 = dz2 @ W2\ndz1 = dh1 * (1 - h1**2)\ngW1 = dz1.T @ xb + 2*lambda_l2*W1\ngb1 = dz1.sum(axis=0)\n\n# Atualiza\u00e7\u00e3o dos par\u00e2metros\nW3 -= lr * gW3; b3 -= lr * gb3\nW2 -= lr * gW2; b2 -= lr * gb2\nW1 -= lr * gW1; b1 -= lr * gb1\n</code></pre> <p>Durante o treinamento, foram adotadas tr\u00eas pr\u00e1ticas fundamentais:</p> <ul> <li>Mini-Batch Training: processa subconjuntos de amostras, aumentando a efici\u00eancia e introduzindo ru\u00eddo estoc\u00e1stico que ajuda na generaliza\u00e7\u00e3o.</li> <li>Regulariza\u00e7\u00e3o L2: evita overfitting penalizando pesos muito grandes.</li> <li>Early Stopping: interrompe o treino se a perda de valida\u00e7\u00e3o (<code>val_loss</code>) n\u00e3o melhorar ap\u00f3s 15 \u00e9pocas consecutivas, preservando o modelo \u00f3timo.</li> </ul>"},{"location":"#monitoramento-de-metricas","title":"Monitoramento de m\u00e9tricas","text":"<p>Durante as \u00e9pocas, foram registrados vetores de perda e acur\u00e1cia para treino e valida\u00e7\u00e3o:</p> <ul> <li><code>train_losses</code>, <code>val_losses</code></li> <li><code>train_accs</code>, <code>val_accs</code></li> </ul> <p>Exemplo dos logs registrados:</p> <pre><code>\u00c9poca  10 | TrainLoss 0.5499 | ValLoss 0.5394 | TrainAcc 0.793 | ValAcc 0.791\n\u00c9poca 200 | TrainLoss 0.4619 | ValLoss 0.4555 | TrainAcc 0.826 | ValAcc 0.823\nEarly stopping (\u00e9poca 219) melhor val_loss=0.4528\n</code></pre> <p>Esses logs mostram uma converg\u00eancia suave, com perda diminuindo at\u00e9 a \u00e9poca ~200, quando a valida\u00e7\u00e3o estabiliza, demonstrando equil\u00edbrio entre aprendizado e generaliza\u00e7\u00e3o.</p>"},{"location":"#geracao-das-curvas-de-aprendizado","title":"Gera\u00e7\u00e3o das curvas de aprendizado","text":"<p>As curvas de loss e acur\u00e1cia foram plotadas para inspe\u00e7\u00e3o visual da converg\u00eancia:</p> <pre><code>plt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss\"); plt.title(\"Curva de Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(train_accs, label=\"Train Acc\")\nplt.plot(val_accs, label=\"Val Acc\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\"); plt.title(\"Curva de Acur\u00e1cia\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Esses gr\u00e1ficos permitem verificar se h\u00e1 overfitting (diverg\u00eancia entre treino e valida\u00e7\u00e3o) ou underfitting (valores de perda altos e est\u00e1veis).</p>"},{"location":"#avaliacao-e-checkpoint","title":"Avalia\u00e7\u00e3o e checkpoint","text":"<p>Ao final do treino:</p> <ul> <li>O modelo \u00e9 restaurado para os pesos do melhor checkpoint (menor <code>val_loss</code>);</li> <li>Avaliado no conjunto de teste com m\u00e9tricas: acur\u00e1cia, precis\u00e3o, recall e F1-score;</li> <li>Uma matriz de confus\u00e3o \u00e9 gerada para an\u00e1lise dos erros;</li> <li>O decision boundary \u00e9 visualizado no espa\u00e7o 2D via PCA.</li> </ul>"},{"location":"#5-model-training","title":"5. Model Training","text":""},{"location":"#51-training-setup","title":"5.1 Training Setup","text":"<ul> <li>Dados j\u00e1 codificados e escalados (MinMax).</li> <li>Split feito com <code>stratify</code> para preservar propor\u00e7\u00f5es de classe (70% train / 15% val / 15% test).</li> <li>Shuffle por \u00e9poca com <code>rng.permutation</code>.</li> </ul>"},{"location":"#52-training-loop-resumo","title":"5.2 Training Loop (resumo)","text":"<ul> <li> <p>Para cada \u00e9poca:</p> </li> <li> <p>embaralha treino;</p> </li> <li>itera por mini-batches;</li> <li>forward \u2192 calcula <code>loss</code> (+ L2) \u2192 backward \u2192 atualiza par\u00e2metros;</li> <li>ao fim da \u00e9poca calcula <code>train_loss</code>, <code>val_loss</code>, <code>train_acc</code>, <code>val_acc</code>.</li> <li>Early stopping: interrompe se <code>val_loss</code> n\u00e3o melhorar por <code>patience</code> \u00e9pocas.</li> </ul> <p>Trechos chave (exemplo do forward, loss e update):</p> <pre><code># forward (mini-batch)\nz1 = xb @ W1.T + b1; h1 = tanh(z1)\nz2 = h1 @ W2.T + b2; h2 = tanh(z2)\nz3 = h2 @ W3.T + b3; yhat = softmax(z3)\n\n# loss CE + L2\nloss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))\nl2_term = lambda_l2 * (np.sum(W1*W1) + ...)\nloss = loss_ce + l2_term\n\n# backward (resumo)\nd3 = (yhat - yb) / B\ngW3 = d3.T @ h2 + 2*lambda_l2*W3\n... # calcula gW2,gW1 e gb*\n\n# update\nW3 -= lr * gW3\n...\n</code></pre>"},{"location":"#53-training-observations-logs","title":"5.3 Training Observations (logs)","text":"<p>Trechos de log produzidos durante treinamento:</p> <pre><code>\u00c9poca   1 | TrainLoss 0.9766 | ValLoss 0.8791 | TrainAcc 0.654 | ValAcc 0.653\n\u00c9poca  10 | TrainLoss 0.5499 | ValLoss 0.5394 | TrainAcc 0.793 | ValAcc 0.791\n...\n\u00c9poca 210 | TrainLoss 0.4615 | ValLoss 0.4534 | TrainAcc 0.825 | ValAcc 0.823\nEarly stopping (\u00e9poca 219) melhor val_loss=0.4528\n</code></pre> <ul> <li>Observa-se r\u00e1pida converg\u00eancia nas primeiras 50 \u00e9pocas; depois estabiliza\u00e7\u00e3o com pequenas melhorias.</li> <li>Early stopping ao redor de ~219 \u00e9pocas.</li> </ul>"},{"location":"#6-training-and-testing-strategy","title":"6. Training and Testing Strategy","text":""},{"location":"#61-data-split","title":"6.1 Data Split","text":"<ul> <li><code>train / val / test = 70% / 15% / 15%</code> com stratify para preservar distribui\u00e7\u00e3o de classes.</li> </ul>"},{"location":"#62-validation-strategy","title":"6.2 Validation Strategy","text":"<ul> <li>Valida\u00e7\u00e3o simples (conjunto de valida\u00e7\u00e3o dedicado) usada para sele\u00e7\u00e3o de hiperpar\u00e2metros e early stopping.</li> </ul>"},{"location":"#63-reproducibility","title":"6.3 Reproducibility","text":"<ul> <li>Seed fixada via <code>np.random.default_rng(42)</code> e <code>random_state=42</code> nos <code>train_test_split</code>.</li> </ul>"},{"location":"#64-overfitting-prevention","title":"6.4 Overfitting Prevention","text":"<ul> <li>Regulariza\u00e7\u00e3o L2 em todos os pesos.</li> <li>Early stopping baseado em <code>val_loss</code>.</li> <li>Batch training (mini-batch) fornece ru\u00eddo \u00fatil ao otimizar.</li> </ul>"},{"location":"#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":""},{"location":"#71-loss-and-accuracy-curves","title":"7.1 Loss and Accuracy Curves","text":"<p>Figura: Curva de Loss e Acur\u00e1cia (treino vs valida\u00e7\u00e3o).</p> <p></p>"},{"location":"#72-analysis-of-learning-behavior","title":"7.2 Analysis of Learning Behavior","text":"<ul> <li>Treino e valida\u00e7\u00e3o converge pr\u00f3ximos e sem diverg\u00eancia acentuada \u2192 overfitting limitado gra\u00e7as a L2 + early stopping.</li> <li>Acur\u00e1cia de valida\u00e7\u00e3o estabilizou ~0.823, indicando bom ajuste do modelo.</li> </ul>"},{"location":"#8-evaluation-metrics","title":"8. Evaluation Metrics","text":""},{"location":"#81-test-metrics-resultado-final","title":"8.1 Test Metrics (resultado final)","text":"<p>Resultados calculados no conjunto de teste:</p> <pre><code>M\u00c9TRICAS TESTE\nAcur\u00e1cia : 0.8195\nPrecis\u00e3o : 0.7897\nRecall   : 0.7690\nF1-score : 0.7771\n</code></pre>"},{"location":"#82-baseline-majority-class","title":"8.2 Baseline (majority class)","text":"<p>Classe majorit\u00e1ria no teste: <code>2</code> Baseline (predizer sempre a classe majorit\u00e1ria):</p> <pre><code>M\u00c9TRICAS BASELINE (Majority Class)\nAcur\u00e1cia : 0.4708\nPrecis\u00e3o : 0.1569\nRecall   : 0.3333\nF1-score : 0.2134\n</code></pre> <p>Compara\u00e7\u00e3o: o MLP supera claramente o baseline em todas as m\u00e9tricas (ex.: acur\u00e1cia 0.8195 vs 0.4708).</p>"},{"location":"#83-confusion-matrix","title":"8.3 Confusion Matrix","text":"<p>Figura: Matriz de Confus\u00e3o (Teste):</p> <p></p> <p>Classe \"Graduate (1.0)\" O modelo obteve o melhor desempenho nesta classe, com 4088 acertos e poucos erros de confus\u00e3o. Representa tamb\u00e9m a classe majorit\u00e1ria, o que explica a facilidade de identifica\u00e7\u00e3o. Poucos graduados foram confundidos como \u201cDropout\u201d (242) ou \u201cEnrolled\u201d (84).</p> <p>Classe \"Enrolled (0.5)\" A segunda melhor classe em termos de acerto (2568 corretos). Alguns alunos ainda matriculados foram incorretamente classificados como \u201cDropout\u201d (341) \u2014 possivelmente devido a perfis de desempenho inicial similares. Menor confus\u00e3o com \u201cGraduate\u201d (205) indica que o modelo diferencia razoavelmente alunos ativos dos formados.</p> <p>Classe \"Dropout (0.0)\" A classe mais problem\u00e1tica: apenas 1028 acertos contra 820 erros (211+609). Quase 600 alunos que abandonaram o curso foram classificados como \u201cGraduate\u201d \u2014 mostrando que o modelo tem dificuldade em detectar evas\u00e3o, o que \u00e9 cr\u00edtico neste tipo de aplica\u00e7\u00e3o. Essa confus\u00e3o indica que o modelo tende a superestimar o sucesso acad\u00eamico, possivelmente por conta do desbalanceamento do dataset.</p>"},{"location":"#84-decision-boundary-pca-2d","title":"8.4 Decision Boundary (PCA 2D)","text":"<p>Figura: Decis\u00e3o no espa\u00e7o PCA (PC1 \u00d7 PC2) com pontos corretos/errados do conjunto de teste.</p> <p></p>"},{"location":"#9-conclusion","title":"9. Conclusion","text":""},{"location":"#91-key-findings","title":"9.1 Key Findings","text":"<ul> <li>MLP implementado do zero (NumPy) atingiu Acur\u00e1cia = 0.8195 e F1 \u2248 0.7771 no conjunto de teste, superando largamente o baseline majorit\u00e1rio.</li> <li>PCA mostrou separa\u00e7\u00e3o parcial entre classes, justificando uso de modelo n\u00e3o-linear (MLP).</li> </ul>"},{"location":"#92-limitations","title":"9.2 Limitations","text":"<ul> <li>Desbalanceamento de classes requer an\u00e1lise adicional (class weighting, oversampling/undersampling).</li> <li>Remo\u00e7\u00e3o de outliers foi feita por limites manuais \u2014 abordagem autom\u00e1tica (IQR, isolation forest) poderia ser comparada.</li> <li>Implementa\u00e7\u00e3o atual usa <code>tanh</code>; testar <code>ReLU</code> + batchnorm pode acelerar/aperfei\u00e7oar treino.</li> </ul>"},{"location":"#10-referencias","title":"10. Refer\u00eancias","text":"<ol> <li> <p>Material de apoio \u2014 Artificial Neural Networks and Deep Learning https://insper.github.io/ann-dl/</p> </li> <li> <p>Kaggle \u2014 Academic Success Classifier Competition https://www.kaggle.com/competitions/academic-success-classifier/data</p> </li> <li> <p>UCI Machine Learning Repository \u2014 Predict Students\u2019 Dropout and Academic Success https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success</p> </li> </ol>"},{"location":"#appendix-codigo-de-treino-e-avaliacao","title":"Appendix \u2014 C\u00f3digo de Treino e Avalia\u00e7\u00e3o","text":"<p>Arquivo Jupyter implementando o MLP .</p>"}]}